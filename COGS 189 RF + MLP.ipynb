{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **DeceptionGame Dataset Analysis Part 1**"
      ],
      "metadata": {
        "id": "VIiV7G2jJ8l9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrtupwHDPR7t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VStlRd_XO_Ob"
      },
      "source": [
        "#### **Download Data**\n",
        "This should take some time, the dataset is very large even preprocessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6uEuNIo-T-Z",
        "outputId": "6be2c92f-61e4-4576-93db-16b919bda280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-21 20:35:08--  https://figshare.com/ndownloader/files/43502097\n",
            "Resolving figshare.com (figshare.com)... 52.17.159.36, 52.30.109.106, 52.49.42.6, ...\n",
            "Connecting to figshare.com (figshare.com)|52.17.159.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/43502097/Preprocessed.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20250321/eu-west-1/s3/aws4_request&X-Amz-Date=20250321T203509Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=91127827496419c9fa2b68cdb19a04522e79fc3cf90fae7bf2e495736e2da1e9 [following]\n",
            "--2025-03-21 20:35:09--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/43502097/Preprocessed.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20250321/eu-west-1/s3/aws4_request&X-Amz-Date=20250321T203509Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=91127827496419c9fa2b68cdb19a04522e79fc3cf90fae7bf2e495736e2da1e9\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.92.33.56, 52.218.106.187, 52.218.109.235, ...\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.92.33.56|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1453534919 (1.4G) [application/zip]\n",
            "Saving to: ‘preprocessed_dataset.zip’\n",
            "\n",
            "preprocessed_datase 100%[===================>]   1.35G  11.0MB/s    in 2m 13s  \n",
            "\n",
            "2025-03-21 20:37:23 (10.5 MB/s) - ‘preprocessed_dataset.zip’ saved [1453534919/1453534919]\n",
            "\n",
            "Archive:  preprocessed_dataset.zip\n",
            "   creating: Preprocessed/\n",
            "   creating: Preprocessed/DecisionMaking/\n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub01_Observer_sub02.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub09_Observer_sub10.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub17_Observer_sub18.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub03_Observer_sub06.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub13_Observer_sub14.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub10_Observer_sub09.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub20_Observer_sub21.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub14_Observer_sub13.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub04_Observer_sub05.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub07_Observer_sub08.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub18_Observer_sub17.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub23_Observer_sub24.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub08_Observer_sub07.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub11_Observer_sub12.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub15_Observer_sub16.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub24_Observer_sub23.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub12_Observer_sub11.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub22_Observer_sub19.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub21_Observer_sub20.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub16_Observer_sub15.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub05_Observer_sub04.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub19_Observer_sub22.mat  \n",
            "  inflating: Preprocessed/DecisionMaking/Player_sub06_Observer_sub03.mat  \n",
            "   creating: Preprocessed/Feedback/\n",
            "  inflating: Preprocessed/Feedback/Player_sub01_Observer_sub02.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub09_Observer_sub10.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub17_Observer_sub18.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub03_Observer_sub06.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub13_Observer_sub14.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub10_Observer_sub09.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub20_Observer_sub21.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub14_Observer_sub13.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub04_Observer_sub05.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub07_Observer_sub08.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub18_Observer_sub17.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub23_Observer_sub24.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub08_Observer_sub07.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub11_Observer_sub12.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub15_Observer_sub16.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub24_Observer_sub23.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub12_Observer_sub11.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub22_Observer_sub19.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub21_Observer_sub20.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub16_Observer_sub15.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub05_Observer_sub04.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub19_Observer_sub22.mat  \n",
            "  inflating: Preprocessed/Feedback/Player_sub06_Observer_sub03.mat  \n"
          ]
        }
      ],
      "source": [
        "!wget -O preprocessed_dataset.zip \"https://figshare.com/ndownloader/files/43502097\"\n",
        "!unzip preprocessed_dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK88_jmH9rAU"
      },
      "source": [
        "#### **Understanding Data Structure**\n",
        "Here, we perform some exploratory commands to get a better grasp of the shape and form of the data. We then load it in and extract the instructed truth and lie data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9VZsEM-PdYt",
        "outputId": "9072f9eb-d5ec-4a17-f6f5-b20e46a8a4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['__header__', '__version__', '__globals__', 'observer', 'player'])\n"
          ]
        }
      ],
      "source": [
        "# Load .mat file\n",
        "mat_data = sio.loadmat('/content/Preprocessed/DecisionMaking/Player_sub01_Observer_sub02.mat') #if needed, replace with appropriate path\n",
        "\n",
        "print(mat_data.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0s7QlY_Pj75",
        "outputId": "4dc972b3-deb1-4bc5-e852-8fb4981e425c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ],
      "source": [
        "# check the data type of 'observer' and 'player', these should be numpy arrays\n",
        "print(type(mat_data['observer']))\n",
        "print(type(mat_data['player']))\n",
        "\n",
        "# Extract observer and player data\n",
        "observer_data = mat_data['observer']\n",
        "player_data = mat_data['player']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m4UezQUFxLO",
        "outputId": "f484bb92-eb12-4a3f-830f-fb0491011383"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "type(player_data[0][0][1]) #this should also be a numpy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv0eHynC9iMV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f92f8aba-7266-481e-cfe8-33921e945381"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-7.18764830e+00,  1.22919858e+00,  1.81678867e+00, ...,\n",
              "         -3.17998007e-02,  7.14602321e-03,  4.38691902e+00],\n",
              "        [-6.71852255e+00,  1.22490931e+00,  2.86546040e+00, ...,\n",
              "          7.31438875e-01,  1.42970324e+00,  6.39665174e+00],\n",
              "        [-5.53971100e+00, -1.23877919e+00,  7.66455460e+00, ...,\n",
              "          8.11694801e-01, -1.58020723e+00,  4.86925459e+00],\n",
              "        ...,\n",
              "        [-3.65176725e+00, -9.83069777e-01,  7.05432653e+00, ...,\n",
              "         -3.05836296e+00, -2.73539591e+00,  4.22301978e-01],\n",
              "        [-4.28767014e+00, -6.27262235e-01,  3.57845688e+00, ...,\n",
              "         -2.18507266e+00,  2.50094247e+00,  1.80136287e+00],\n",
              "        [-1.57916006e-02,  1.03468634e-02,  3.39718556e+00, ...,\n",
              "         -2.53226328e+00, -4.62264919e+00,  1.25445604e+00]],\n",
              "\n",
              "       [[-5.54400253e+00, -1.55448639e+00, -3.51589417e+00, ...,\n",
              "         -5.55847597e+00, -2.27181697e+00, -1.18284665e-01],\n",
              "        [-6.30074787e+00, -5.78233778e-01,  5.55397391e-01, ...,\n",
              "         -3.71414137e+00,  6.15843654e-01,  2.65817451e+00],\n",
              "        [-4.65939665e+00, -2.45546007e+00,  1.35086858e+00, ...,\n",
              "         -4.83176613e+00, -2.75780797e+00,  3.00967979e+00],\n",
              "        ...,\n",
              "        [-6.53916454e+00,  2.89503503e+00,  1.47248459e+00, ...,\n",
              "         -2.47479248e+00, -2.05746388e+00,  1.13692021e+00],\n",
              "        [-2.84730244e+00,  3.04552197e-01, -1.75463498e+00, ...,\n",
              "         -3.48439264e+00,  3.90784883e+00, -1.50499952e+00],\n",
              "        [ 1.17208719e+00, -3.61312723e+00,  3.60294402e-01, ...,\n",
              "          1.01256752e+01,  4.54454851e+00, -2.48313951e+00]],\n",
              "\n",
              "       [[-7.35538483e+00, -1.44181997e-01, -2.98033404e+00, ...,\n",
              "         -6.68597841e+00, -1.47512615e+00,  1.51473582e+00],\n",
              "        [-5.67991781e+00, -2.92824388e+00,  8.13927129e-02, ...,\n",
              "          1.81183934e+00,  9.49546933e-01, -4.22870922e+00],\n",
              "        [-4.15523624e+00, -8.71130917e-03,  1.31465328e+00, ...,\n",
              "         -8.99759889e-01,  9.06029046e-01, -1.28414118e+00],\n",
              "        ...,\n",
              "        [-5.63066912e+00,  2.29743695e+00,  1.03841400e+00, ...,\n",
              "         -8.06910419e+00, -2.68088150e+00,  2.36509943e+00],\n",
              "        [-3.80768943e+00,  2.32160115e+00,  2.62059331e+00, ...,\n",
              "         -4.63634968e+00, -2.83466792e+00,  2.88867116e+00],\n",
              "        [ 2.10567379e+00,  2.98235393e+00,  1.02260828e+00, ...,\n",
              "         -1.34420323e+00, -1.28258657e+00, -2.38266611e+00]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 8.92868996e-01, -8.79051495e+00, -1.31889045e-01, ...,\n",
              "         -2.06159854e+00, -3.64154148e+00,  1.36794591e+00],\n",
              "        [-3.97095770e-01, -4.75482082e+00, -8.05790186e-01, ...,\n",
              "          3.84356213e+00, -3.13111472e+00, -2.19773316e+00],\n",
              "        [-5.22835207e+00, -2.46980000e+00,  1.07134497e+00, ...,\n",
              "          2.54057097e+00, -2.86648560e+00, -1.10490978e+00],\n",
              "        ...,\n",
              "        [ 2.70679498e+00, -1.12748251e+01,  5.54011631e+00, ...,\n",
              "         -1.08576369e+00, -5.77381563e+00,  4.69760084e+00],\n",
              "        [ 8.97381544e-01, -9.35299587e+00,  2.94346380e+00, ...,\n",
              "          1.23731160e+00, -5.65249062e+00,  4.39181328e+00],\n",
              "        [ 2.64440536e-01, -9.69195175e+00,  6.62748528e+00, ...,\n",
              "         -5.78931952e+00, -4.16345119e+00,  1.07883501e+01]],\n",
              "\n",
              "       [[-2.80731893e+00, -7.05710697e+00,  2.17389607e+00, ...,\n",
              "         -8.17479038e+00, -1.21271932e+00,  2.08576488e+00],\n",
              "        [-6.30840969e+00,  8.26375127e-01,  1.46418512e-01, ...,\n",
              "         -2.07363307e-01, -1.84930444e-01,  3.89146161e+00],\n",
              "        [-6.66321945e+00,  1.39529079e-01,  3.88515258e+00, ...,\n",
              "         -2.59280658e+00, -1.05894697e+00,  2.18802214e+00],\n",
              "        ...,\n",
              "        [-4.41369772e-01, -1.01295891e+01,  4.72203493e+00, ...,\n",
              "         -1.94373107e+00, -4.14938736e+00, -1.75346470e+00],\n",
              "        [ 1.08131623e+00, -7.18088531e+00,  1.60724521e+00, ...,\n",
              "         -4.12223911e+00, -1.55507386e+00, -7.70420492e-01],\n",
              "        [ 7.54346085e+00, -8.72051334e+00,  2.76399827e+00, ...,\n",
              "          4.68183947e+00, -1.35460663e+00, -3.90271592e+00]],\n",
              "\n",
              "       [[-4.11760300e-01, -4.51869965e+00,  3.96758199e+00, ...,\n",
              "          2.66198611e+00,  1.32230723e+00,  2.68714809e+00],\n",
              "        [-1.92060435e+00,  4.50615835e+00, -6.57444000e-01, ...,\n",
              "         -2.04636526e+00,  1.17130458e+00,  2.60393810e+00],\n",
              "        [-5.41937685e+00,  2.91202831e+00,  2.92536616e+00, ...,\n",
              "         -7.23196745e-01, -7.07757413e-01,  2.01746488e+00],\n",
              "        ...,\n",
              "        [ 2.48821473e+00, -3.45526099e+00,  9.18978310e+00, ...,\n",
              "          1.81899917e+00, -3.83909988e+00,  1.86663294e+00],\n",
              "        [ 2.09084725e+00, -5.38526678e+00,  3.88818669e+00, ...,\n",
              "          1.88057566e+00, -1.56888437e+00,  7.05448449e-01],\n",
              "        [ 5.61189461e+00, -9.41087532e+00,  3.14412856e+00, ...,\n",
              "          1.65883803e+00, -3.32191730e+00, -7.62783349e-01]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "x = player_data[0][0][1]\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/Preprocessed/DecisionMaking\"\n",
        "mat_files = [f for f in os.listdir(data_dir) if f.endswith(\".mat\")]\n",
        "\n",
        "\n",
        "all_x, all_y = [], []\n",
        "\n",
        "\n",
        "for file in mat_files:\n",
        "    mat_data = sio.loadmat(os.path.join(data_dir, file))\n",
        "\n",
        "    player_data = mat_data['player'][0, 0]\n",
        "\n",
        "    # Identify Instructed Lie and Instructed Truth trials\n",
        "    instructed_lie_trials = np.where(player_data['y'][1, :] == 1)[0]\n",
        "    instructed_truth_trials = np.where(player_data['y'][2, :] == 1)[0]\n",
        "\n",
        "\n",
        "    # Extract EEG data for selected trials and specific channel\n",
        "    x_lie = player_data['x'][:, :, instructed_lie_trials]  # Shape: (timepoints, num of channels, trials)\n",
        "    x_truth = player_data['x'][:, :, instructed_truth_trials]  # Shape: (timepoints,num of channels, trials)\n",
        "    #min_trials = min(x_lie.shape[2], x_truth.shape[2])\n",
        "\n",
        "    x = np.concatenate([x_lie, x_truth], axis=2)  # Combine along trials axis - 2\n",
        "    y = np.concatenate([\n",
        "        np.zeros(len(instructed_lie_trials)),\n",
        "        np.ones(len(instructed_truth_trials))\n",
        "    ])\n",
        "\n",
        "    all_x.append(x)\n",
        "    all_y.append(y)\n"
      ],
      "metadata": {
        "id": "VJBchyheXapy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_data = np.concatenate(all_x, axis=2).transpose(2, 1, 0)  # Combine trials/ samples, channels, time\n",
        "y_data = np.concatenate(all_y)  # Combine labels"
      ],
      "metadata": {
        "id": "9fJn10CHm9fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZNqIzUV98so"
      },
      "source": [
        "### **Model Development**\n",
        "#### **Random Forest**\n",
        "We choose random forest as the first model because of its affinity for high-dimensional data and noise handling."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Model\n",
        "import numpy as np\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.signal import welch\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "def extract_features(data_X):\n",
        "    \"\"\"\n",
        "    Extracts statistical and frequency domain features from EEG signals.\n",
        "    \"\"\"\n",
        "    num_samples, num_channels, num_timesteps = data_X.shape\n",
        "    features = []\n",
        "\n",
        "    for sample in range(num_samples):\n",
        "        sample_features = []\n",
        "        for ch in range(num_channels):\n",
        "            signal = data_X[sample, ch, :]\n",
        "\n",
        "            # Statistical features\n",
        "            sample_features.extend([\n",
        "                np.mean(signal),\n",
        "                np.std(signal),\n",
        "                skew(signal),\n",
        "                kurtosis(signal)\n",
        "            ])\n",
        "\n",
        "            # Frequency domain features using Welch’s method\n",
        "            freqs, psd = welch(signal, fs=100, nperseg=100)  # Assuming EEG sampled at 256 Hz\n",
        "            power_bands = [\n",
        "                np.mean(psd[(freqs >= 1) & (freqs < 4)]),  # Delta\n",
        "                np.mean(psd[(freqs >= 4) & (freqs < 8)]),  # Theta\n",
        "                np.mean(psd[(freqs >= 8) & (freqs < 13)]),  # Alpha\n",
        "                np.mean(psd[(freqs >= 13) & (freqs < 30)]),  # Beta\n",
        "                np.mean(psd[(freqs >= 30) & (freqs < 50)])   # Gamma\n",
        "            ]\n",
        "            sample_features.extend(power_bands)\n",
        "\n",
        "        features.append(sample_features)\n",
        "\n",
        "    return np.array(features)\n",
        "\n",
        "X_features = extract_features(X_data)\n",
        "y_labels = y_data\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Random Forest Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "osMPM1LXXc5h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e25ee4-6c38-4484-cf1c-49e76240b415"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count occurrences of 0 and 1\n",
        "num_zeros = np.sum(y_data == 0)\n",
        "num_ones = np.sum(y_data == 1)\n",
        "\n",
        "print(f\"Number of 0s: {num_zeros}\")\n",
        "print(f\"Number of 1s: {num_ones}\")"
      ],
      "metadata": {
        "id": "aE0IQFQ4RsE4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ba2375-8a48-4815-a7fd-6da8d0152f2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of 0s: 3047\n",
            "Number of 1s: 2782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ERP With MLP Model Development\n",
        "\n",
        "First, we average EEG to compute ERP, which will simplify analysis and suppress randomness. Then, we run the next model, MLP, which we chose because it can model complex relationships between ERP signals and class labels without requiring large datasets. We convert ERP features and labels into tensors, then split with an 80/20 split with batch size 8.\n",
        "\n",
        "The architeture of the model is as follows:\n",
        "\n",
        "Layer 1: 4 -> 64\n",
        "\n",
        "ReLU\n",
        "\n",
        "Layer 2: 64 -> 32\n",
        "\n",
        "ReLU\n",
        "\n",
        "Layer 3: 32 -> 1\n",
        "\n",
        "Sigmoid"
      ],
      "metadata": {
        "id": "C4eSAs0smsEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal import butter, filtfilt\n",
        "\n",
        "def bandpass_filter(data, lowcut=1, highcut=30, fs=100, order=5):\n",
        "    nyquist = 0.5 * fs\n",
        "    low = lowcut / nyquist\n",
        "    high = highcut / nyquist\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data, axis=0)  # Apply along time point axis\n",
        "\n",
        "# Apply filtering\n",
        "X_data_filtered = bandpass_filter(X_data)"
      ],
      "metadata": {
        "id": "gUaOVGvQi525"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_truth = X_data_filtered[y_data == 1]\n",
        "X_lie = X_data_filtered[y_data == 0]\n",
        "\n",
        "# Compute mean ERP for each condition\n",
        "erp_truth = np.mean(X_truth, axis=0)  # Shape: num_channels, timepoints\n",
        "erp_lie = np.mean(X_lie, axis=0)"
      ],
      "metadata": {
        "id": "QsZ3prJ-nA4L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def smooth_erp(erp_data, window_size=10):\n",
        "    \"\"\"\n",
        "    Further smooth the erp_data. you guys can modify the window_size\n",
        "    \"\"\"\n",
        "    num_chnl, num_times = erp_data.shape\n",
        "    smoothed_erp = np.zeros_like(erp_data)\n",
        "\n",
        "    for ch in range(num_chnl):\n",
        "        smoothed_signal = np.convolve(erp_data[ch, :], np.ones(window_size)/window_size, mode='same')\n",
        "        smoothed_erp[ch, :] = smoothed_signal\n",
        "    return smoothed_erp  # Output shape: num_channels, timepoints\n",
        "\n",
        "\n",
        "# Apply smoothing\n",
        "erp_truth_smooth = smooth_erp(erp_truth)\n",
        "erp_lie_smooth = smooth_erp(erp_lie)"
      ],
      "metadata": {
        "id": "TOh2b2MInHbd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fs = 100  # All data is 100 Hz\n",
        "\n",
        "erp_windows = {\n",
        "    \"seg1\": (15, 24),\n",
        "    \"seg2\": (25, 35),\n",
        "    \"seg3\": (35, 45),\n",
        "    \"seg4\": (45, 60),\n",
        "}\n",
        "\n",
        "def extract_erp_features(erp_data):\n",
        "    features = []\n",
        "    for comp, (start, end) in erp_windows.items():\n",
        "        mean_amplitude = np.mean(erp_data[:, start:end], axis=1)  # Mean per component\n",
        "        features.append(mean_amplitude)\n",
        "    return np.array(features).T  # Shape: (num_channels, num_components)\n",
        "\n",
        "# Extract ERP features\n",
        "features_truth = extract_erp_features(erp_truth_smooth)\n",
        "features_lie = extract_erp_features(erp_lie_smooth)\n",
        "\n",
        "# Concatenate\n",
        "X_features = np.concatenate([features_truth, features_lie], axis=0)  # Shape: (num_samples, num_features)\n",
        "y_labels = np.concatenate([np.ones(len(features_truth)), np.zeros(len(features_lie))])"
      ],
      "metadata": {
        "id": "-sYu_b8QndqW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Convert your ERP features and labels into tensors\n",
        "X_features_tensor = torch.tensor(X_features, dtype=torch.float32)  # (60, 4)\n",
        "y_labels_tensor = torch.tensor(y_labels, dtype=torch.float32).unsqueeze(1)  # (60, 1)\n",
        "\n",
        "# Train-test split\n",
        "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(\n",
        "    X_features_tensor, y_labels_tensor, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# Define MLP model\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim=4, hidden_dim=64):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Initialize\n",
        "model = MLPModel()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        preds = model(xb).cpu()\n",
        "        all_preds.append(preds)\n",
        "        all_labels.append(yb)\n",
        "\n",
        "y_pred = torch.cat(all_preds).numpy()\n",
        "y_true = torch.cat(all_labels).numpy()\n",
        "y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_true, y_pred_labels))\n",
        "print(classification_report(y_true, y_pred_labels))"
      ],
      "metadata": {
        "id": "rKbEtkYmoenP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e189df-6a0a-46b3-f5a5-aa234b4905d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50, Loss: 0.6976\n",
            "Epoch 2/50, Loss: 0.6932\n",
            "Epoch 3/50, Loss: 0.6925\n",
            "Epoch 4/50, Loss: 0.6921\n",
            "Epoch 5/50, Loss: 0.6915\n",
            "Epoch 6/50, Loss: 0.6912\n",
            "Epoch 7/50, Loss: 0.6913\n",
            "Epoch 8/50, Loss: 0.6907\n",
            "Epoch 9/50, Loss: 0.6905\n",
            "Epoch 10/50, Loss: 0.6906\n",
            "Epoch 11/50, Loss: 0.6906\n",
            "Epoch 12/50, Loss: 0.6903\n",
            "Epoch 13/50, Loss: 0.6901\n",
            "Epoch 14/50, Loss: 0.6901\n",
            "Epoch 15/50, Loss: 0.6899\n",
            "Epoch 16/50, Loss: 0.6899\n",
            "Epoch 17/50, Loss: 0.6898\n",
            "Epoch 18/50, Loss: 0.6900\n",
            "Epoch 19/50, Loss: 0.6899\n",
            "Epoch 20/50, Loss: 0.6899\n",
            "Epoch 21/50, Loss: 0.6898\n",
            "Epoch 22/50, Loss: 0.6902\n",
            "Epoch 23/50, Loss: 0.6902\n",
            "Epoch 24/50, Loss: 0.6899\n",
            "Epoch 25/50, Loss: 0.6898\n",
            "Epoch 26/50, Loss: 0.6899\n",
            "Epoch 27/50, Loss: 0.6898\n",
            "Epoch 28/50, Loss: 0.6898\n",
            "Epoch 29/50, Loss: 0.6899\n",
            "Epoch 30/50, Loss: 0.6901\n",
            "Epoch 31/50, Loss: 0.6896\n",
            "Epoch 32/50, Loss: 0.6898\n",
            "Epoch 33/50, Loss: 0.6899\n",
            "Epoch 34/50, Loss: 0.6898\n",
            "Epoch 35/50, Loss: 0.6898\n",
            "Epoch 36/50, Loss: 0.6897\n",
            "Epoch 37/50, Loss: 0.6897\n",
            "Epoch 38/50, Loss: 0.6901\n",
            "Epoch 39/50, Loss: 0.6897\n",
            "Epoch 40/50, Loss: 0.6898\n",
            "Epoch 41/50, Loss: 0.6897\n",
            "Epoch 42/50, Loss: 0.6900\n",
            "Epoch 43/50, Loss: 0.6896\n",
            "Epoch 44/50, Loss: 0.6897\n",
            "Epoch 45/50, Loss: 0.6898\n",
            "Epoch 46/50, Loss: 0.6898\n",
            "Epoch 47/50, Loss: 0.6897\n",
            "Epoch 48/50, Loss: 0.6898\n",
            "Epoch 49/50, Loss: 0.6903\n",
            "Epoch 50/50, Loss: 0.6899\n",
            "Accuracy: 0.3333333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00         8\n",
            "         1.0       0.33      1.00      0.50         4\n",
            "\n",
            "    accuracy                           0.33        12\n",
            "   macro avg       0.17      0.50      0.25        12\n",
            "weighted avg       0.11      0.33      0.17        12\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **K-Fold Cross Validation**\n",
        "We use a K-Fold Cross Validation to verify accuracy percentage between different subsets of training and test data."
      ],
      "metadata": {
        "id": "5R5iTQmkM00J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Prepare tensors\n",
        "X_features_tensor = torch.tensor(X_features, dtype=torch.float32)  # Shape: (60, 4)\n",
        "y_labels_tensor = torch.tensor(y_labels, dtype=torch.float32).unsqueeze(1)  # Shape: (60, 1)\n",
        "\n",
        "# Define MLP model\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim=4, hidden_dim=64):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "criterion = nn.BCELoss()\n",
        "num_epochs = 50\n",
        "batch_size = 8\n",
        "k_folds = 5\n",
        "\n",
        "# Stratified K-Fold setup\n",
        "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "y_np = y_labels_tensor.squeeze().numpy()\n",
        "\n",
        "all_fold_accuracies = []\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(skf.split(X_features, y_np)):\n",
        "    print(f\"\\n Fold {fold + 1}/{k_folds}\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test = X_features_tensor[train_idx], X_features_tensor[test_idx]\n",
        "    y_train, y_test = y_labels_tensor[train_idx], y_labels_tensor[test_idx]\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    # Initialize a fresh model\n",
        "    model = MLPModel().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Train loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        # Uncomment to monitor training loss\n",
        "        # print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    y_preds, y_trues = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            output = model(xb).cpu()\n",
        "            y_preds.append(output)\n",
        "            y_trues.append(yb)\n",
        "\n",
        "    y_pred = torch.cat(y_preds).numpy()\n",
        "    y_true = torch.cat(y_trues).numpy()\n",
        "    y_pred_labels = (y_pred > 0.5).astype(int)\n",
        "\n",
        "    acc = accuracy_score(y_true, y_pred_labels)\n",
        "    all_fold_accuracies.append(acc)\n",
        "\n",
        "    print(\"Fold Accuracy:\", acc)\n",
        "    print(classification_report(y_true, y_pred_labels, digits=4))\n",
        "\n",
        "# Summary\n",
        "print(\"\\Average Accuracy over 5 folds:\", np.mean(all_fold_accuracies))"
      ],
      "metadata": {
        "id": "2_Lmy6Dzssu5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18385a00-1c2c-441f-b22a-a6e082fdeacf"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Fold 1/5\n",
            "Fold Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.5000    1.0000    0.6667         6\n",
            "         1.0     0.0000    0.0000    0.0000         6\n",
            "\n",
            "    accuracy                         0.5000        12\n",
            "   macro avg     0.2500    0.5000    0.3333        12\n",
            "weighted avg     0.2500    0.5000    0.3333        12\n",
            "\n",
            "\n",
            " Fold 2/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold Accuracy: 0.6666666666666666\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7500    0.5000    0.6000         6\n",
            "         1.0     0.6250    0.8333    0.7143         6\n",
            "\n",
            "    accuracy                         0.6667        12\n",
            "   macro avg     0.6875    0.6667    0.6571        12\n",
            "weighted avg     0.6875    0.6667    0.6571        12\n",
            "\n",
            "\n",
            " Fold 3/5\n",
            "Fold Accuracy: 0.5833333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     1.0000    0.1667    0.2857         6\n",
            "         1.0     0.5455    1.0000    0.7059         6\n",
            "\n",
            "    accuracy                         0.5833        12\n",
            "   macro avg     0.7727    0.5833    0.4958        12\n",
            "weighted avg     0.7727    0.5833    0.4958        12\n",
            "\n",
            "\n",
            " Fold 4/5\n",
            "Fold Accuracy: 0.5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.5000    1.0000    0.6667         6\n",
            "         1.0     0.0000    0.0000    0.0000         6\n",
            "\n",
            "    accuracy                         0.5000        12\n",
            "   macro avg     0.2500    0.5000    0.3333        12\n",
            "weighted avg     0.2500    0.5000    0.3333        12\n",
            "\n",
            "\n",
            " Fold 5/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold Accuracy: 0.3333333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.3333    0.3333    0.3333         6\n",
            "         1.0     0.3333    0.3333    0.3333         6\n",
            "\n",
            "    accuracy                         0.3333        12\n",
            "   macro avg     0.3333    0.3333    0.3333        12\n",
            "weighted avg     0.3333    0.3333    0.3333        12\n",
            "\n",
            "\\Average Accuracy over 5 folds: 0.5166666666666667\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}